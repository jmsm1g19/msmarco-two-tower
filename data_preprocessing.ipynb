{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "\n",
    "from typing import Tuple, List, Dict\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from pandas.core.frame import DataFrame\n",
    "from tqdm import tqdm  # Add this import for progress tracking\n",
    "import multiprocessing as mp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Processing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unravel_passages(dataset):\n",
    "    # Preallocate lists for better memory efficiency\n",
    "    n_total = sum(len(p['passage_text']) for p in dataset['passages'])\n",
    "    queries = np.empty(n_total, dtype=object)\n",
    "    passages = np.empty(n_total, dtype=object)\n",
    "    urls = np.empty(n_total, dtype=object)\n",
    "    \n",
    "    idx = 0\n",
    "    for i, query in enumerate(dataset['query']):\n",
    "        n_passages = len(dataset['passages'][i]['passage_text'])\n",
    "        queries[idx:idx+n_passages] = [query] * n_passages\n",
    "        passages[idx:idx+n_passages] = dataset['passages'][i]['passage_text']\n",
    "        urls[idx:idx+n_passages] = dataset['passages'][i]['url']\n",
    "        idx += n_passages\n",
    "    \n",
    "    return pd.DataFrame({'query': queries, 'passage': passages, 'url': urls})\n",
    "\n",
    "def pre_sample_irrelevant(all_passages_ids, num_queries, samples_per_query=20):\n",
    "    # Pre-sample irrelevant passages for each query\n",
    "    pre_samples = {query_id: random.sample(all_passages_ids, samples_per_query) \n",
    "                   for query_id in range(num_queries)}\n",
    "    return pre_samples\n",
    "\n",
    "def create_triplets_dataframe(unraveled_data, pre_samples):\n",
    "    # Use vectorized operations instead of apply\n",
    "    relevant_passages = unraveled_data.groupby('query_id')['passage_id'].agg(list).reset_index(name='relevant')\n",
    "    all_passages_ids = set(unraveled_data['passage_id'])\n",
    "    \n",
    "    # Vectorized filtering using numpy operations\n",
    "    relevant_passages['irrelevant'] = relevant_passages.apply(\n",
    "        lambda row: np.setdiff1d(\n",
    "            pre_samples[row['query_id']], \n",
    "            row['relevant']\n",
    "        )[:len(row['relevant'])].tolist(),\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    return relevant_passages\n",
    "\n",
    "def prepare_mappings_optim(unraveled_data):\n",
    "    unique_queries = pd.DataFrame({'query': unraveled_data['query'].unique()})\n",
    "    unique_passages = pd.DataFrame({'passage': unraveled_data['passage'].unique()})\n",
    "    unique_queries['query_id'] = unique_queries.index\n",
    "    unique_passages['passage_id'] = unique_passages.index\n",
    "    return unique_queries, unique_passages\n",
    "\n",
    "def map_ids(\n",
    "    unraveled_data: DataFrame,\n",
    "    unique_queries: DataFrame,\n",
    "    unique_passages: DataFrame\n",
    ") -> DataFrame:\n",
    "    \"\"\"Optimized mapping of IDs using hash joins.\"\"\"\n",
    "    # Use more efficient merge strategy\n",
    "    return (unraveled_data\n",
    "            .merge(unique_queries, on='query', how='left', copy=False)\n",
    "            .merge(unique_passages, on='passage', how='left', copy=False))\n",
    "\n",
    "def expand_triplets(triplets_df: DataFrame) -> DataFrame:\n",
    "    \"\"\"Vectorized triplet expansion.\"\"\"\n",
    "    # Create expanded arrays\n",
    "    query_ids = np.repeat(triplets_df['query_id'].values, \n",
    "                         triplets_df['relevant'].str.len())\n",
    "    \n",
    "    # Flatten the lists using list comprehension\n",
    "    positives = [p for sublist in triplets_df['relevant'] for p in sublist]\n",
    "    negatives = [n for sublist in triplets_df['irrelevant'] for n in sublist]\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'query_id': query_ids,\n",
    "        'positive_passage_id': positives,\n",
    "        'negative_passage_id': negatives\n",
    "    })\n",
    "    \n",
    "def process_dataset(dataset_split):\n",
    "    # Add parallel processing for large datasets\n",
    "    with mp.Pool(mp.cpu_count()) as pool:\n",
    "        unraveled_data = unravel_passages(dataset_split)\n",
    "        unique_queries, unique_passages = prepare_mappings_optim(unraveled_data)\n",
    "        \n",
    "        # Parallel processing for ID mapping\n",
    "        chunk_size = len(unraveled_data) // mp.cpu_count()\n",
    "        chunks = [unraveled_data[i:i + chunk_size] for i in range(0, len(unraveled_data), chunk_size)]\n",
    "        \n",
    "        mapped_chunks = pool.starmap(\n",
    "            map_ids,\n",
    "            [(chunk, unique_queries, unique_passages) for chunk in chunks]\n",
    "        )\n",
    "        unraveled_data = pd.concat(mapped_chunks)\n",
    "    \n",
    "    pre_samples = pre_sample_irrelevant(\n",
    "        list(set(unraveled_data['passage_id'])), \n",
    "        unique_queries.shape[0]\n",
    "    )\n",
    "    triplets_df = create_triplets_dataframe(unraveled_data, pre_samples)\n",
    "    return triplets_df, unique_queries, unique_passages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"ms_marco\", \"v1.1\", split='train', streaming=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': ['Results-Based Accountability is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole.'], 'passages': {'is_selected': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0], 'passage_text': [\"Since 2007, the RBA's outstanding reputation has been affected by the 'Securency' or NPA scandal. These RBA subsidiaries were involved in bribing overseas officials so that Australia might win lucrative note-printing contracts. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", \"The Reserve Bank of Australia (RBA) came into being on 14 January 1960 as Australia 's central bank and banknote issuing authority, when the Reserve Bank Act 1959 removed the central banking functions from the Commonwealth Bank. The assets of the bank include the gold and foreign exchange reserves of Australia, which is estimated to have a net worth of A$101 billion. Nearly 94% of the RBA's employees work at its headquarters in Sydney, New South Wales and at the Business Resumption Site.\", 'RBA Recognized with the 2014 Microsoft US Regional Partner of the ... by PR Newswire. Contract Awarded for supply and support the. Securitisations System used for risk management and analysis. ', 'The inner workings of a rebuildable atomizer are surprisingly simple. The coil inside the RBA is made of some type of resistance wire, normally Kanthal or nichrome. When a current is applied to the coil (resistance wire), it heats up and the heated coil then vaporizes the eliquid. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'Results-Based Accountability® (also known as RBA) is a disciplined way of thinking and taking action that communities can use to improve the lives of children, youth, families, adults and the community as a whole. RBA is also used by organizations to improve the performance of their programs. Creating Community Impact with RBA. Community impact focuses on conditions of well-being for children, families and the community as a whole that a group of leaders is working collectively to improve. For example: “Residents with good jobs,” “Children ready for school,” or “A safe and clean neighborhood”.', 'RBA uses a data-driven, decision-making process to help communities and organizations get beyond talking about problems to taking action to solve problems. It is a simple, common sense framework that everyone can understand. RBA starts with ends and works backward, towards means. The “end” or difference you are trying to make looks slightly different if you are working on a broad community level or are focusing on your specific program or organization. RBA improves the lives of children, families, and communities and the performance of programs because RBA: 1  Gets from talk to action quickly; 2  Is a simple, common sense process that everyone can understand; 3  Helps groups to surface and challenge assumptions that can be barriers to innovation;', 'vs. NetIQ Identity Manager. Risk-based authentication (RBA) is a method of applying varying levels of stringency to authentication processes based on the likelihood that access to a given system could result in its being compromised. Risk-based authentication can be categorized as either user-dependent or transaction-dependent. User-dependent RBA processes employ the same authentication for every session initiated by a given user; the exact credentials that the site demands depend on who the user is.', 'A rebuildable atomizer (RBA), often referred to as simply a “rebuildable,” is just a special type of atomizer used in the Vape Pen and Mod Industry that connects to a personal vaporizer. 1 The bottom feed RBA is, perhaps, the easiest of all RBA types to build, maintain, and use. 2  It is filled from below, much like bottom coil clearomizer. 3  Bottom feed RBAs can utilize cotton instead of silica for the wick. 4  The Genesis, or genny, is a top feed RBA that utilizes a short woven mesh wire.', 'Get To Know Us. RBA is a digital and technology consultancy with roots in strategy, design and technology. Our team of specialists help progressive companies deliver modern digital experiences backed by proven technology engineering. '], 'url': ['https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia', 'https://en.wikipedia.org/wiki/Reserve_Bank_of_Australia', 'http://acronyms.thefreedictionary.com/RBA', 'https://www.slimvapepen.com/rebuildable-atomizer-rba/', 'http://rba-africa.com/about/what-is-rba/', 'http://resultsleadership.org/what-is-results-based-accountability-rba/', 'http://rba-africa.com/about/what-is-rba/', 'http://searchsecurity.techtarget.com/definition/risk-based-authentication-RBA', 'https://www.slimvapepen.com/rebuildable-atomizer-rba/', 'http://www.rbaconsulting.com/']}, 'query': 'what is rba', 'query_id': 19699, 'query_type': 'description', 'wellFormedAnswers': []}\n"
     ]
    }
   ],
   "source": [
    "print(next(iter(dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    validation: Dataset({\n",
       "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
       "        num_rows: 10047\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
       "        num_rows: 82326\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['answers', 'passages', 'query', 'query_id', 'query_type', 'wellFormedAnswers'],\n",
       "        num_rows: 9650\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answers': ['Approximately $15,000 per year.'],\n",
       " 'passages': {'is_selected': [1, 0, 0, 0, 0, 0],\n",
       "  'passage_text': ['The average Walgreens salary ranges from approximately $15,000 per year for Customer Service Associate / Cashier to $179,900 per year for District Manager. Average Walgreens hourly pay ranges from approximately $7.35 per hour for Laboratory Technician to $68.90 per hour for Pharmacy Manager. Salary information comes from 7,810 data points collected directly from employees, users, and jobs on Indeed.',\n",
       "   'The average revenue in 2011 of a Starbuck Store was $1,078,000, up  from $1,011,000 in 2010.    The average ticket (total purchase) at domestic Starbuck stores in  No … vember 2007 was reported at $6.36.    In 2008, the average ticket was flat (0.0% change).',\n",
       "   'In fiscal 2014, Walgreens opened a total of 184 new locations and acquired 84 locations, for a net decrease of 273 after relocations and closings. How big are your stores? The average size for a typical Walgreens is about 14,500 square feet and the sales floor averages about 11,000 square feet. How do we select locations for new stores? There are several factors that Walgreens takes into account, such as major intersections, traffic patterns, demographics and locations near hospitals.',\n",
       "   'th store in 1984, reaching $4 billion in sales in 1987, and $5 billion two years later. Walgreens ended the 1980s with 1,484 stores, $5.3 billion in revenues and $154 million in profits. However, profit margins remained just below 3 percent of sales, and returns on assets of less than 10 percent.',\n",
       "   'The number of Walgreen stores has risen from 5,000 in 2005 to more than 8,000 at present. The average square footage per store stood at approximately 10,200 and we forecast the figure to remain constant over our review period. Walgreen earned $303 as average front-end revenue per store square foot in 2012.',\n",
       "   'Your Walgreens Store. Select a store from the search results to make it Your Walgreens Store and save time getting what you need. Your Walgreens Store will be the default location for picking up prescriptions, photos, in store orders and finding deals in the Weekly Ad.'],\n",
       "  'url': ['http://www.indeed.com/cmp/Walgreens/salaries',\n",
       "   \"http://www.answers.com/Q/What_is_the_average_gross_sales_volume_of_a_single_Walgreen's_Store\",\n",
       "   'http://news.walgreens.com/fact-sheets/frequently-asked-questions.htm',\n",
       "   'http://www.babson.edu/executive-education/thought-leadership/retailing/Documents/walgreens-strategic-evolution.pdf',\n",
       "   'http://www.trefis.com/stock/wag/articles/199532/key-trends-driving-walgreens-business/2013-08-07',\n",
       "   'http://www.walgreens.com/storelocator/find.jsp?requestType=locator']},\n",
       " 'query': 'walgreens store sales average',\n",
       " 'query_id': 9652,\n",
       " 'query_type': 'numeric',\n",
       " 'wellFormedAnswers': []}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['validation'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_triplets, train_queries, train_passages \u001b[38;5;241m=\u001b[39m process_dataset(pd\u001b[38;5;241m.\u001b[39mDataFrame(dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/pandas/core/frame.py:843\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    841\u001b[0m         data \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(data)\n\u001b[1;32m    842\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 843\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(data)\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_dataclass(data[\u001b[38;5;241m0\u001b[39m]):\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/datasets/arrow_dataset.py:2440\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2438\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m   2439\u001b[0m             pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2440\u001b[0m             formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2441\u001b[0m                 pa_subtable_ex,\n\u001b[1;32m   2442\u001b[0m                 \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2443\u001b[0m                 formatter\u001b[38;5;241m=\u001b[39mformatter,\n\u001b[1;32m   2444\u001b[0m                 format_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m   2445\u001b[0m                 output_all_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m   2446\u001b[0m             )\n\u001b[1;32m   2447\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[1;32m   2448\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/datasets/formatting/formatting.py:633\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    631\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 633\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[38;5;241m=\u001b[39mquery_type)\n\u001b[1;32m    634\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    635\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/datasets/formatting/formatting.py:397\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 397\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_row(pa_table)\n\u001b[1;32m    398\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    399\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/datasets/formatting/formatting.py:437\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 437\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_arrow_extractor()\u001b[38;5;241m.\u001b[39mextract_row(pa_table)\n\u001b[1;32m    438\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/datasets/formatting/formatting.py:145\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 145\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(pa_table\u001b[38;5;241m.\u001b[39mto_pydict())\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/pyarrow/table.pxi:2157\u001b[0m, in \u001b[0;36mpyarrow.lib._Tabular.to_pydict\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/pyarrow/table.pxi:1335\u001b[0m, in \u001b[0;36mpyarrow.lib.ChunkedArray.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/pyarrow/array.pxi:1607\u001b[0m, in \u001b[0;36mpyarrow.lib.Array.to_pylist\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/two-tower/lib/python3.12/site-packages/pyarrow/scalar.pxi:793\u001b[0m, in \u001b[0;36mpyarrow.lib.StructScalar.as_py\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m<frozen _collections_abc>:819\u001b[0m, in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_triplets, train_queries, train_passages = process_dataset(pd.DataFrame(dataset['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_triplets, test_queries, test_passages = process_dataset(pd.DataFrame(dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_triplets, val_queries, val_passages = process_dataset(pd.DataFrame(dataset['validation']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_triplets.to_parquet('../data/mappings/train_triplets_compressed.parquet', index=False)\n",
    "# train_queries.to_parquet('../data/mappings/train_queries.parquet', index=False)\n",
    "# train_passages.to_parquet('../data/mappings/train_passages.parquet', index=False)\n",
    "\n",
    "# test_triplets.to_parquet('../data/mappings/test_triplets_compressed.parquet', index=False)\n",
    "# test_queries.to_parquet('../data/mappings/test_queries.parquet', index=False)\n",
    "# test_passages.to_parquet('../data/mappings/test_passages.parquet', index=False)\n",
    "\n",
    "# val_triplets.to_parquet('../data/mappings/val_triplets_compressed.parquet', index=False)\n",
    "# val_queries.to_parquet('../data/mappings/val_queries.parquet', index=False)\n",
    "# val_passages.to_parquet('../data/mappings/val_passages.parquet', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_triplets_exp = expand_triplets(train_triplets)\n",
    "test_triplets_exp = expand_triplets(test_triplets)\n",
    "val_triplets_exp = expand_triplets(val_triplets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_triplets_exp.to_parquet('../data/mappings/train_triplets_expanded.parquet', index=False)\n",
    "# test_triplets_exp.to_parquet('../data/mappings/test_triplets_expanded.parquet', index=False)\n",
    "# val_triplets_exp.to_parquet('../data/mappings/val_triplets_expanded.parquet', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "two-tower",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
